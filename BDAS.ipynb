{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BDAS').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = spark.read.csv(\"E:\\INFOSYS 722\\Assignments\\Assignments\\Iter4\\Iteration4\\credit_card_Dataset2.csv\",\n",
    "                       inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+----+-----------+------------------+\n",
      "|RecordID|Location|  ip| CSV|Contactless|Card_Name_Accurecy|\n",
      "+--------+--------+----+----+-----------+------------------+\n",
      "|       1|     136|7136| Yes|         No|                34|\n",
      "|       2|      74| 674| Yes|        Yes|                72|\n",
      "|       3|    null|3198|null|        Yes|                25|\n",
      "|       4|     194| 194|  No|        Yes|                10|\n",
      "|       5|    null|null|null|        Yes|                90|\n",
      "|       6|     106| 106| Yes|         No|                38|\n",
      "|       7|     202|9202|  No|         No|                51|\n",
      "|       8|     245|4245| Yes|         No|                83|\n",
      "|       9|      14| 914| Yes|         No|                41|\n",
      "|      10|      93| 993| Yes|        Yes|              null|\n",
      "|      11|     196|5196|  No|         No|                49|\n",
      "|      12|     151|1151|  No|        Yes|              null|\n",
      "|      13|      76| 576|  No|        Yes|                32|\n",
      "|      14|     165|6165| Yes|         No|                55|\n",
      "|      15|    null| 371|null|         No|                19|\n",
      "|      16|     210|8210| Yes|        Yes|                68|\n",
      "|      17|     251|6251|  No|        Yes|                69|\n",
      "|      18|      58| 958|  No|         No|                17|\n",
      "|      19|     250|7250|  No|         No|                44|\n",
      "|      20|     136|2136|  No|         No|                93|\n",
      "+--------+--------+----+----+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+-----+-----------+------------------+\n",
      "|RecordID|Location|   ip|  CSV|Contactless|Card_Name_Accurecy|\n",
      "+--------+--------+-----+-----+-----------+------------------+\n",
      "|       0|   44346|19043|19033|      19029|            258879|\n",
      "+--------+--------+-----+-----+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "ds2_null = ds2.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            col(c).contains('NaN') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in ds2.columns])\n",
    "ds2_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+----+-----------+------------------+\n",
      "|RecordID|Location|  ip| CSV|Contactless|Card_Name_Accurecy|\n",
      "+--------+--------+----+----+-----------+------------------+\n",
      "|       1|     136|7136| Yes|         No|                34|\n",
      "|       2|      74| 674| Yes|        Yes|                72|\n",
      "|       3|    null|3198|null|        Yes|                25|\n",
      "|       4|     194| 194|  No|        Yes|                10|\n",
      "|       5|    null|null|null|        Yes|                90|\n",
      "|       6|     106| 106| Yes|         No|                38|\n",
      "|       7|     202|9202|  No|         No|                51|\n",
      "|       8|     245|4245| Yes|         No|                83|\n",
      "|       9|      14| 914| Yes|         No|                41|\n",
      "|      10|      93| 993| Yes|        Yes|              null|\n",
      "+--------+--------+----+----+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2=ds2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+---+-----------+------------------+\n",
      "|RecordID|Location|  ip|CSV|Contactless|Card_Name_Accurecy|\n",
      "+--------+--------+----+---+-----------+------------------+\n",
      "|       1|     136|7136|Yes|         No|                34|\n",
      "|       2|      74| 674|Yes|        Yes|                72|\n",
      "|       4|     194| 194| No|        Yes|                10|\n",
      "|       6|     106| 106|Yes|         No|                38|\n",
      "|       7|     202|9202| No|         No|                51|\n",
      "|       8|     245|4245|Yes|         No|                83|\n",
      "|       9|      14| 914|Yes|         No|                41|\n",
      "|      11|     196|5196| No|         No|                49|\n",
      "|      13|      76| 576| No|        Yes|                32|\n",
      "|      14|     165|6165|Yes|         No|                55|\n",
      "+--------+--------+----+---+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+---+-----------+------------------+\n",
      "|RecordID|Location|  ip|CSV|Contactless|Card_Name_Accurecy|\n",
      "+--------+--------+----+---+-----------+------------------+\n",
      "|       1|     136|7136|Yes|         No|                 1|\n",
      "|       2|      74| 674|Yes|        Yes|                 3|\n",
      "|       4|     194| 194| No|        Yes|                 1|\n",
      "|       6|     106| 106|Yes|         No|                 2|\n",
      "|       7|     202|9202| No|         No|                 2|\n",
      "|       8|     245|4245|Yes|         No|                 3|\n",
      "|       9|      14| 914|Yes|         No|                 2|\n",
      "|      11|     196|5196| No|         No|                 2|\n",
      "|      13|      76| 576| No|        Yes|                 1|\n",
      "|      14|     165|6165|Yes|         No|                 2|\n",
      "+--------+--------+----+---+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "ds11=ds2\n",
    "ds11=ds11.withColumn(\"Card_Name_Accurecy\", when(ds11[\"Card_Name_Accurecy\"] < 35, 1).otherwise(ds11[\"Card_Name_Accurecy\"]))\n",
    "ds11=ds11.withColumn(\"Card_Name_Accurecy\", when(ds11[\"Card_Name_Accurecy\"] >= 70, 3).otherwise(ds11[\"Card_Name_Accurecy\"]))\n",
    "ds11=ds11.withColumn(\"Card_Name_Accurecy\", when((ds11[\"Card_Name_Accurecy\"] >= 35) & (ds11[\"Card_Name_Accurecy\"] <70), 2).otherwise(ds11[\"Card_Name_Accurecy\"]))\n",
    "ds11.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = spark.read.csv(\"E:\\INFOSYS 722\\Assignments\\Assignments\\Iter4\\Iteration4\\credit_card_Dataset1.csv\",\n",
    "                       inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordID: integer (nullable = true)\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.csv(\"E:\\INFOSYS 722\\Assignments\\Assignments\\Iter4\\Iteration4\\CleanData.csv\",\n",
    "                       inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1|  492|\n",
      "|    0|  492|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = scaledData.select(\"scaledFeatures\", \"Class\")\n",
    "final_data=final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
    "final_data=final_data.withColumnRenamed(\"Class\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(\"features\", \"Class\")\n",
    "final_data=final_data.withColumnRenamed(\"Class\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.10%\n",
      "weighted Precision = 93.13%\n",
      "weighted Recall = 93.10%\n",
      "F1 = 93.10%\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3],seed=1234)\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 1 (classes)\n",
    "#layers = [30,16,2]\n",
    "layers = [30,16,2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(\n",
    "                                        tol=1E-4,\n",
    "                                        layers=layers, \n",
    "                                        maxIter=150, \n",
    "                                        blockSize=128, \n",
    "                                        seed=1234)\n",
    "# train the model\n",
    "model = trainer.fit(train_data)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test_data)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "\n",
    "\n",
    "evaluator1 = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "evaluator2 = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "evaluator3 = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "evaluator4 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "\n",
    "print(\"Accuracy = \" +  \"{:.2%}\".format(evaluator1.evaluate(predictionAndLabels)));\n",
    "print(\"weighted Precision = \" + \"{:.2%}\".format(evaluator2.evaluate(predictionAndLabels)));\n",
    "print(\"weighted Recall = \" + \"{:.2%}\".format(evaluator3.evaluate(predictionAndLabels)));\n",
    "print(\"F1 = \" + \"{:.2%}\".format(evaluator4.evaluate(predictionAndLabels)));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[114781.0,0.01868...|    0|\n",
      "|[48085.0,-1.17863...|    0|\n",
      "|[84380.0,1.116270...|    0|\n",
      "|[80374.0,-0.42396...|    0|\n",
      "|[68284.0,0.083971...|    0|\n",
      "|[127819.0,2.04654...|    0|\n",
      "|[80775.0,-0.66946...|    0|\n",
      "|[124360.0,1.61669...|    0|\n",
      "|[125685.0,1.94626...|    0|\n",
      "|[142746.0,1.87460...|    0|\n",
      "|[68322.0,-0.46750...|    0|\n",
      "|[74903.0,1.243635...|    0|\n",
      "|[62814.0,1.252808...|    0|\n",
      "|[834.0,0.95624797...|    0|\n",
      "|[73282.0,-0.33819...|    0|\n",
      "|[22305.0,1.041925...|    0|\n",
      "|[128772.0,1.31398...|    0|\n",
      "|[146618.0,1.95809...|    0|\n",
      "|[19912.0,1.362146...|    0|\n",
      "|[157659.0,-0.3994...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(final_data)\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(final_data, {normalizer.p: float(\"inf\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|        normFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|[114781.0,0.01868...|    0|[1.0,1.6283070368...|\n",
      "|[48085.0,-1.17863...|    0|[1.0,-2.451144381...|\n",
      "|[84380.0,1.116270...|    0|[1.0,1.3229088516...|\n",
      "|[80374.0,-0.42396...|    0|[1.0,-5.274858586...|\n",
      "|[68284.0,0.083971...|    0|[1.0,1.2297452258...|\n",
      "|[127819.0,2.04654...|    0|[1.0,1.6011248335...|\n",
      "|[80775.0,-0.66946...|    0|[1.0,-8.288058891...|\n",
      "|[124360.0,1.61669...|    0|[1.0,1.3000151214...|\n",
      "|[125685.0,1.94626...|    0|[1.0,1.5485269976...|\n",
      "|[142746.0,1.87460...|    0|[1.0,1.3132421258...|\n",
      "|[68322.0,-0.46750...|    0|[1.0,-6.842668715...|\n",
      "|[74903.0,1.243635...|    0|[1.0,1.6603284260...|\n",
      "|[62814.0,1.252808...|    0|[1.0,1.9944736810...|\n",
      "|[834.0,0.95624797...|    0|[1.0,0.0011465803...|\n",
      "|[73282.0,-0.33819...|    0|[1.0,-4.614987937...|\n",
      "|[22305.0,1.041925...|    0|[1.0,4.6712642277...|\n",
      "|[128772.0,1.31398...|    0|[1.0,1.0203937284...|\n",
      "|[146618.0,1.95809...|    0|[1.0,1.3355076900...|\n",
      "|[19912.0,1.362146...|    0|[1.0,6.8408321313...|\n",
      "|[157659.0,-0.3994...|    0|[1.0,-2.533917093...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(final_data)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(final_data)\n",
    "fd = scaledData.select(\"scaledFeatures\", \"label\")\n",
    "fd=fd.withColumnRenamed(\"scaledFeatures\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[2.39859819743112...|    0|\n",
      "|[1.00484047293084...|    0|\n",
      "|[1.76330329844868...|    0|\n",
      "|[1.67958923097315...|    0|\n",
      "|[1.42694243222647...|    0|\n",
      "|[2.67105551439218...|    0|\n",
      "|[1.68796899658914...|    0|\n",
      "|[2.59877219951503...|    0|\n",
      "|[2.62646095123871...|    0|\n",
      "|[2.98298758758421...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.10%\n",
      "weighted Precision = 93.13%\n",
      "weighted Recall = 93.10%\n",
      "F1 = 93.10%\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = fd.randomSplit([0.7,0.3],seed=1234)\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 1 (classes)\n",
    "#layers = [30,16,2]\n",
    "layers = [30,16,2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(\n",
    "                                        tol=1E-3,\n",
    "                                        layers=layers, \n",
    "                                        maxIter=150, \n",
    "                                        blockSize=128, \n",
    "                                        seed=1234)\n",
    "# train the model\n",
    "model = trainer.fit(train_data)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test_data)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "\n",
    "\n",
    "evaluator1 = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "evaluator2 = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "evaluator3 = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "evaluator4 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "\n",
    "print(\"Accuracy = \" +  \"{:.2%}\".format(evaluator1.evaluate(predictionAndLabels)));\n",
    "print(\"weighted Precision = \" + \"{:.2%}\".format(evaluator2.evaluate(predictionAndLabels)));\n",
    "print(\"weighted Recall = \" + \"{:.2%}\".format(evaluator3.evaluate(predictionAndLabels)));\n",
    "print(\"F1 = \" + \"{:.2%}\".format(evaluator4.evaluate(predictionAndLabels)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
